{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis notebook presents a version of the model based on the transformer architecture. The model is trained on a dataset consisting of a variety of dialogs. At the end of the notebook is the model inference code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n\nimport numpy as np\nimport codecs\nimport math\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:30:05.555123Z","iopub.execute_input":"2023-06-07T22:30:05.555469Z","iopub.status.idle":"2023-06-07T22:30:05.560841Z","shell.execute_reply.started":"2023-06-07T22:30:05.555441Z","shell.execute_reply":"2023-06-07T22:30:05.559979Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing Pipeline","metadata":{}},{"cell_type":"code","source":"# READING THE DATA\nlines = open('.../Chatbot/movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\nconversations = open('.../Chatbot/movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:00:37.438388Z","iopub.execute_input":"2023-06-07T22:00:37.438958Z","iopub.status.idle":"2023-06-07T22:00:38.160211Z","shell.execute_reply.started":"2023-06-07T22:00:37.438922Z","shell.execute_reply":"2023-06-07T22:00:38.159260Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Creating a dictionary that maps each line and its id\nid2line = {}\nfor line in lines:\n    _line = line.split(' +++$+++ ')\n    if len(_line) == 5:\n        id2line[_line[0]] = _line[4]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:00:38.161587Z","iopub.execute_input":"2023-06-07T22:00:38.161943Z","iopub.status.idle":"2023-06-07T22:00:38.508830Z","shell.execute_reply.started":"2023-06-07T22:00:38.161910Z","shell.execute_reply":"2023-06-07T22:00:38.507771Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Creating a list of all of the conversations\nconversations_ids = []\nfor conversation in conversations[:-1]:\n    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n    conversations_ids.append(_conversation.split(','))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:00:38.511446Z","iopub.execute_input":"2023-06-07T22:00:38.512238Z","iopub.status.idle":"2023-06-07T22:00:38.867254Z","shell.execute_reply.started":"2023-06-07T22:00:38.512199Z","shell.execute_reply":"2023-06-07T22:00:38.866293Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Getting separately the questions and the answers\nquestions = []\nanswers = []\nfor conversation in conversations_ids:\n    for i in range(len(conversation) - 1):\n        questions.append(id2line[conversation[i]])\n        answers.append(id2line[conversation[i+1]])","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:00:38.868511Z","iopub.execute_input":"2023-06-07T22:00:38.868866Z","iopub.status.idle":"2023-06-07T22:00:39.107257Z","shell.execute_reply.started":"2023-06-07T22:00:38.868834Z","shell.execute_reply":"2023-06-07T22:00:39.106352Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Doing a first cleaning of the texts\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:00:39.108591Z","iopub.execute_input":"2023-06-07T22:00:39.109040Z","iopub.status.idle":"2023-06-07T22:00:39.126276Z","shell.execute_reply.started":"2023-06-07T22:00:39.108990Z","shell.execute_reply":"2023-06-07T22:00:39.125281Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Cleaning the questions\nclean_questions = []\nfor question in questions:\n    clean_questions.append(clean_text(question))\n\n# Cleaning the answers\nclean_answers = []\nfor answer in answers:\n    clean_answers.append(clean_text(answer))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:00:39.128047Z","iopub.execute_input":"2023-06-07T22:00:39.128674Z","iopub.status.idle":"2023-06-07T22:00:50.541774Z","shell.execute_reply.started":"2023-06-07T22:00:39.128641Z","shell.execute_reply":"2023-06-07T22:00:50.540688Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Filtering out the questions and answers that are too short or too long\nshort_questions = []\nshort_answers = []\ni = 0\nfor question in clean_questions:\n    if 2 <= len(question.split()) <= 25:\n        short_questions.append(question)\n        short_answers.append(clean_answers[i])\n    i += 1\nclean_questions = []\nclean_answers = []\ni = 0\nfor answer in short_answers:\n    if 2 <= len(answer.split()) <= 25:\n        clean_answers.append(answer)\n        clean_questions.append(short_questions[i])\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:00:50.543243Z","iopub.execute_input":"2023-06-07T22:00:50.543812Z","iopub.status.idle":"2023-06-07T22:00:51.131343Z","shell.execute_reply.started":"2023-06-07T22:00:50.543778Z","shell.execute_reply":"2023-06-07T22:00:51.130414Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# We create a dictionary of matching index and word and the reverse of it.\nvocab = sorted(list(set(''.join(clean_questions + clean_answers))))\n#  Add special tokens\nvocab.append(\"<SOS>\")\nvocab.append(\"<EOS>\")\nvocab.append(\"<PAD>\")\nvocab.append(\"<OUT>\")\nVOCAB_SIZE = len(vocab)\n\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = {i:u for i, u in enumerate(vocab)}\n# Create encoding and decoding functions\nencode = lambda s: [char2idx[c] for c in s]\ndecode = lambda l: ''.join([idx2char[int(i)] for i in l])","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:00:51.132674Z","iopub.execute_input":"2023-06-07T22:00:51.133139Z","iopub.status.idle":"2023-06-07T22:00:51.354947Z","shell.execute_reply.started":"2023-06-07T22:00:51.133104Z","shell.execute_reply":"2023-06-07T22:00:51.354044Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Translate all question and answer characters in dialogs into indexes\ninput_questions_as_int = [encode(question) for question in clean_questions]\ntarget_answers_as_int = [encode(answer) for answer in clean_answers]\n\nencoder_input_seqs = input_questions_as_int\ndecoder_input_seqs = []\ndecoder_target_seqs = []\n# Add the special token indices to the decoder_input_seqs and the decoder_target_seqs\nfor target_text in target_answers_as_int:\n    decoder_input_seqs.append([char2idx['<SOS>']] + target_text)\n    decoder_target_seqs.append(target_text + [char2idx['<EOS>']])","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:00:51.358350Z","iopub.execute_input":"2023-06-07T22:00:51.358732Z","iopub.status.idle":"2023-06-07T22:00:53.822368Z","shell.execute_reply.started":"2023-06-07T22:00:51.358697Z","shell.execute_reply":"2023-06-07T22:00:53.821400Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# determine the maximum number of sequences (question-answer) of data for training \nSEQS = 15000\n\nencoder_input_seqs = encoder_input_seqs[:SEQS]\ndecoder_input_seqs = decoder_input_seqs[:SEQS]\ndecoder_target_seqs = decoder_target_seqs[:SEQS]","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:00:57.614097Z","iopub.execute_input":"2023-06-07T22:00:57.614532Z","iopub.status.idle":"2023-06-07T22:00:57.725010Z","shell.execute_reply.started":"2023-06-07T22:00:57.614496Z","shell.execute_reply":"2023-06-07T22:00:57.724106Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Class for pytorch DataLoader \nclass ChatDataset(Dataset):\n    def __init__(self, encoder_input_seqs, decoder_input_seqs, decoder_target_seqs):\n        self.encoder_input_seqs = encoder_input_seqs\n        self.decoder_input_seqs = decoder_input_seqs\n        self.decoder_target_seqs = decoder_target_seqs\n\n    def __len__(self):\n        return len(self.encoder_input_seqs)\n\n    def __getitem__(self, idx):\n        encoder_input = self.encoder_input_seqs[idx]\n        decoder_input = self.decoder_input_seqs[idx]\n        decoder_target = self.decoder_target_seqs[idx]\n        return encoder_input, decoder_input, decoder_target\n# Function for converting lists into tensors and converting all encoder_inputs, \n# decoder_inputs, decoder_targets sequences to the same length of each group using paddings\npadding_value = char2idx['<PAD>']\ndef collate_fn(batch, padding_value=char2idx['<PAD>']):\n    encoder_inputs, decoder_inputs, decoder_targets = zip(*batch)\n    encoder_inputs = [torch.LongTensor(seq) for seq in encoder_inputs]\n    decoder_inputs = [torch.LongTensor(seq) for seq in decoder_inputs]\n    decoder_targets = [torch.LongTensor(seq) for seq in decoder_targets]\n\n    d_list = encoder_inputs + decoder_inputs + decoder_targets\n    d_list_padded = pad_sequence(d_list, batch_first=True, padding_value=padding_value)\n    encoder_inputs = d_list_padded[:int(1/3 * len(d_list_padded))]\n    decoder_inputs = d_list_padded[int(1/3 * len(d_list_padded)):int(2/3 * len(d_list_padded))]\n    decoder_targets = d_list_padded[int(2/3 * len(d_list_padded)):]\n\n    return encoder_inputs, decoder_inputs, decoder_targets","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:10:05.964848Z","iopub.execute_input":"2023-06-07T23:10:05.965238Z","iopub.status.idle":"2023-06-07T23:10:05.976625Z","shell.execute_reply.started":"2023-06-07T23:10:05.965208Z","shell.execute_reply":"2023-06-07T23:10:05.975651Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# Mask creation func\ndef create_self_attention_mask(sequence_length):\n    # Create a square matrix (sequence_length, sequence_length)\n    mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1).bool() \n    return mask","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:01:03.418847Z","iopub.execute_input":"2023-06-07T22:01:03.419889Z","iopub.status.idle":"2023-06-07T22:01:03.425381Z","shell.execute_reply.started":"2023-06-07T22:01:03.419848Z","shell.execute_reply":"2023-06-07T22:01:03.424192Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size, num_layers, num_heads, dropout):\n        super().__init__()\n        self.encoder = TransformerEncoder(input_size, hidden_size, num_layers, num_heads, dropout)\n        self.decoder = TransformerDecoder(input_size, hidden_size, num_layers, num_heads, dropout)\n        self.output_linear = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, source, target):\n        encoder_output = self.encoder(source)\n        decoder_output = self.decoder(target, encoder_output)\n        output = self.output_linear(decoder_output)\n        return output\n    \nclass TransformerEncoder(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.pos_encoding = PositionalEncoding(hidden_size, dropout)\n        self.encoder_layers = nn.ModuleList([\n            TransformerEncoderLayer(hidden_size, num_heads, dropout)\n            for _ in range(num_layers)\n        ])\n    \n    def forward(self, x):\n             \n        out = self.embedding(x)\n        out = self.pos_encoding(out)\n        \n        for layer in self.encoder_layers:\n            out = layer(out)\n\n        return out\n    \nclass TransformerDecoder(nn.Module):\n    def __init__(self, output_size, hidden_size, num_layers, num_heads, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.pos_encoding = PositionalEncoding(hidden_size, dropout)\n        self.decoder_layers = nn.ModuleList([\n            TransformerDecoderLayer(hidden_size, num_heads, dropout)\n            for _ in range(num_layers)\n        ])\n        \n    def forward(self, x, encoder_output):\n\n        out = self.embedding(x)\n        out = self.pos_encoding(out)\n        \n        mask = create_self_attention_mask(x.size()[1])\n        mask = mask.to(device)\n        \n        for layer in self.decoder_layers:\n            out = layer(out, encoder_output, self_attention_mask=mask)\n\n        return out \n\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, hidden_size, num_heads, dropout):\n        super().__init__()\n        self.self_attention = MultiHeadAttention(hidden_size, num_heads)\n        self.feed_forward = FeedForward(hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm1 = nn.LayerNorm(hidden_size)\n        self.layer_norm2 = nn.LayerNorm(hidden_size)\n    \n    def forward(self, x):\n\n        residual = x # for Add & Norm 1\n        out = self.dropout(self.self_attention(x, x, x))\n        out = residual + out # Add\n        out = self.layer_norm1(out) # Norm\n        \n        residual = out # for Add & Norm 2\n        out = self.dropout(self.feed_forward(out))\n        out = residual + out # Add\n        out = self.layer_norm2(out) # Norm\n\n        return out\n    \nclass TransformerDecoderLayer(nn.Module):\n    def __init__(self, hidden_size, num_heads, dropout):\n        super().__init__()\n        self.self_attention = MultiHeadAttention(hidden_size, num_heads)\n        self.encoder_attention = MultiHeadAttention(hidden_size, num_heads)\n        self.feed_forward = FeedForward(hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm1 = nn.LayerNorm(hidden_size)\n        self.layer_norm2 = nn.LayerNorm(hidden_size)        \n        self.layer_norm3 = nn.LayerNorm(hidden_size)    \n        \n    def forward(self, x, encoder_output, self_attention_mask=None):\n\n        residual = x # for Add & Norm 1\n        out = self.dropout(self.self_attention(x, x, x, mask=self_attention_mask))\n        out = residual + out # Add \n        out = self.layer_norm1(out) # Norm\n        \n        residual = out # for Add & Norm 2\n        out = self.dropout(self.encoder_attention(x, encoder_output, encoder_output)) # СДЕЛАТЬ ПОМЕТКУ q, k, v\n        out = residual + out # Add\n        out = self.layer_norm2(out) # Norm\n        \n        residual = out # for Add & Norm 3\n        out = self.dropout(self.feed_forward(out))\n        out = residual + out # Add\n        out = self.layer_norm3(out) # Norm\n\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, hidden_size, num_heads):\n        super().__init__()\n        self.num_heads = num_heads\n        self.hidden_size = hidden_size\n        self.head_size = hidden_size // num_heads\n        \n        self.q_linear = nn.Linear(hidden_size, hidden_size) \n        self.k_linear = nn.Linear(hidden_size, hidden_size) \n        self.v_linear = nn.Linear(hidden_size, hidden_size) \n        self.output_linear = nn.Linear(hidden_size, hidden_size) \n    \n    def forward(self, query, key, value, mask=None):\n\n        batch_size = query.size(0)\n        \n        q = self.q_linear(query)\n        k = self.k_linear(key)\n        v = self.v_linear(value)\n        \n        q = self._split_heads(q) \n        k = self._split_heads(k)\n        v = self._split_heads(v)\n\n        scores  = torch.matmul(q, k.transpose(-2, -1)) # (-2, -1) dimensions that need to be swapped out \n        scores = scores / (self.head_size ** 0.5) \n        \n        if mask is not None:\n            \n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        scores = F.softmax(scores, dim=-1) \n        \n        attention = torch.matmul(scores, v) \n        attention = self._combine_heads(attention) # сoncatenation of all self-attention heads\n        attention = self.output_linear(attention) \n        \n        return attention\n    \n    # Dividing the hidden_size dimension into self.num_heads and self.heads_size\n    def _split_heads(self, x):\n\n        batch_size, sequence_length, hidden_size = x.size()\n        x = x.view(batch_size, sequence_length, self.num_heads, self.head_size) # .view() change the dimensions without changing the contents\n        x = x.transpose(1, 2)\n        x = x.contiguous().view(batch_size * self.num_heads, sequence_length, self.head_size) # .contiguous() create a new tensor with the same content, but with guaranteed consistent placement of elements in memory.\n\n        return x\n    # Combining the dimensions of self.num_heads and self.heads_size into hidden_size\n    def _combine_heads(self, x):\n        batch_size, sequence_length, head_size = x.size()\n        x = x.view(batch_size // self.num_heads, sequence_length, self.num_heads * head_size)\n        x = x.transpose(1, 2)\n        x = x.contiguous().view(batch_size // self.num_heads, sequence_length, self.hidden_size) \n        return x\n    \nclass FeedForward(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.linear1 = nn.Linear(hidden_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, hidden_size)\n        \n    def forward(self, x):\n        out = F.relu(self.linear1(x))\n        out = self.linear2(out)\n        return out\n    \nclass PositionalEncoding(nn.Module):\n    def __init__(self, hidden_size, dropout, max_length=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        \n        position = torch.arange(0, max_length).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, hidden_size, 2) * -(math.log(10000.0) / hidden_size))\n        pe = torch.zeros(max_length, hidden_size)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T22:01:07.022455Z","iopub.execute_input":"2023-06-07T22:01:07.022820Z","iopub.status.idle":"2023-06-07T22:01:07.058572Z","shell.execute_reply.started":"2023-06-07T22:01:07.022789Z","shell.execute_reply":"2023-06-07T22:01:07.057553Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Define model and training parameters\ninput_size = VOCAB_SIZE\noutput_size = VOCAB_SIZE\nhidden_size = 512\nnum_layers = 2\nnum_heads = 2\ndropout = 0.1\nlearning_rate = 0.001\nnum_epochs = 1000\nbatch_size = 256","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:29:27.040494Z","iopub.execute_input":"2023-06-07T23:29:27.040850Z","iopub.status.idle":"2023-06-07T23:29:27.045697Z","shell.execute_reply.started":"2023-06-07T23:29:27.040821Z","shell.execute_reply":"2023-06-07T23:29:27.044695Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"markdown","source":"The model includes two encoder parts and two decoders (num_layers). The number of \"heads\" in the self-attention mechanisms in the model is 2. In addition, we add 0.1 dropout in some network layers. ","metadata":{}},{"cell_type":"code","source":"model = Transformer(input_size, output_size, hidden_size, num_layers, num_heads, dropout)","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:29:32.675403Z","iopub.execute_input":"2023-06-07T23:29:32.675755Z","iopub.status.idle":"2023-06-07T23:29:32.779665Z","shell.execute_reply.started":"2023-06-07T23:29:32.675726Z","shell.execute_reply":"2023-06-07T23:29:32.778716Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# Loading model parameters from a file\nmodel.load_state_dict(torch.load('.../transformer_model.pth', map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))","metadata":{"execution":{"iopub.status.busy":"2023-06-07T23:29:38.529557Z","iopub.execute_input":"2023-06-07T23:29:38.529914Z","iopub.status.idle":"2023-06-07T23:29:38.610814Z","shell.execute_reply.started":"2023-06-07T23:29:38.529884Z","shell.execute_reply":"2023-06-07T23:29:38.609762Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# Define the loss function and the optimizer\n\ncriterion = nn.CrossEntropyLoss(ignore_index=padding_value)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Create an instance of dataset and data loader\ndataset = ChatDataset(encoder_input_seqs, decoder_input_seqs, decoder_target_seqs)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n# Define the device on which the model will be trained\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:00:53.821505Z","iopub.execute_input":"2023-06-08T00:00:53.821869Z","iopub.status.idle":"2023-06-08T00:00:53.836987Z","shell.execute_reply.started":"2023-06-08T00:00:53.821838Z","shell.execute_reply":"2023-06-08T00:00:53.836114Z"},"trusted":true},"execution_count":121,"outputs":[{"execution_count":121,"output_type":"execute_result","data":{"text/plain":"Transformer(\n  (encoder): TransformerEncoder(\n    (embedding): Embedding(51, 512)\n    (pos_encoding): PositionalEncoding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder_layers): ModuleList(\n      (0-1): 2 x TransformerEncoderLayer(\n        (self_attention): MultiHeadAttention(\n          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (feed_forward): FeedForward(\n          (linear1): Linear(in_features=512, out_features=512, bias=True)\n          (linear2): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (decoder): TransformerDecoder(\n    (embedding): Embedding(51, 512)\n    (pos_encoding): PositionalEncoding(\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (decoder_layers): ModuleList(\n      (0-1): 2 x TransformerDecoderLayer(\n        (self_attention): MultiHeadAttention(\n          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (encoder_attention): MultiHeadAttention(\n          (q_linear): Linear(in_features=512, out_features=512, bias=True)\n          (k_linear): Linear(in_features=512, out_features=512, bias=True)\n          (v_linear): Linear(in_features=512, out_features=512, bias=True)\n          (output_linear): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (feed_forward): FeedForward(\n          (linear1): Linear(in_features=512, out_features=512, bias=True)\n          (linear2): Linear(in_features=512, out_features=512, bias=True)\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n        (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (layer_norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n  )\n  (output_linear): Linear(in_features=512, out_features=51, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Put the model in training mode\nmodel.train()\n\n# Learning Cycle\nfor epoch in range(num_epochs):\n    total_loss = 0\n\n    for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n\n        optimizer.zero_grad()\n\n#         Moving the training data to the device\n        encoder_inputs = encoder_inputs.to(device)\n        decoder_inputs = decoder_inputs.to(device)\n        decoder_targets = decoder_targets.to(device)\n\n        output = model(encoder_inputs, decoder_inputs)\n\n        loss = criterion(output.view(-1, output.size(-1)), decoder_targets.view(-1))\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:02:31.103490Z","iopub.execute_input":"2023-06-08T00:02:31.103844Z","iopub.status.idle":"2023-06-08T00:03:20.417765Z","shell.execute_reply.started":"2023-06-08T00:02:31.103815Z","shell.execute_reply":"2023-06-08T00:03:20.416474Z"},"trusted":true},"execution_count":123,"outputs":[{"name":"stdout","text":"Epoch 1/1000, Loss: 0.0014218665892258286\nEpoch 2/1000, Loss: 0.00129849708173424\nEpoch 3/1000, Loss: 0.0014939273241907358\nEpoch 4/1000, Loss: 0.0013027298264205456\nEpoch 5/1000, Loss: 0.0016055175801739097\nEpoch 6/1000, Loss: 0.001914448570460081\nEpoch 7/1000, Loss: 0.0011589827481657267\nEpoch 8/1000, Loss: 0.0014935389626771212\nEpoch 9/1000, Loss: 0.0013569393195211887\nEpoch 10/1000, Loss: 0.001656110631301999\nEpoch 11/1000, Loss: 0.001419675420038402\nEpoch 12/1000, Loss: 0.0014812697190791368\nEpoch 13/1000, Loss: 0.0012659479398280382\nEpoch 14/1000, Loss: 0.0012508045183494687\nEpoch 15/1000, Loss: 0.0012413456570357084\nEpoch 16/1000, Loss: 0.0014478665543720126\nEpoch 17/1000, Loss: 0.002195980167016387\nEpoch 18/1000, Loss: 0.0019240211695432663\nEpoch 19/1000, Loss: 0.0013073196168988943\nEpoch 20/1000, Loss: 0.0014760562917217612\nEpoch 21/1000, Loss: 0.0019652368500828743\nEpoch 22/1000, Loss: 0.00129566784016788\nEpoch 23/1000, Loss: 0.0020235490519553423\nEpoch 24/1000, Loss: 0.001219764119014144\nEpoch 25/1000, Loss: 0.0020283979829400778\nEpoch 26/1000, Loss: 0.001171826384961605\nEpoch 27/1000, Loss: 0.0013405780773609877\nEpoch 28/1000, Loss: 0.0016893651336431503\nEpoch 29/1000, Loss: 0.0014188919449225068\nEpoch 30/1000, Loss: 0.0011165214236825705\nEpoch 31/1000, Loss: 0.0010390683310106397\nEpoch 32/1000, Loss: 0.001252698595635593\nEpoch 33/1000, Loss: 0.001250157249160111\nEpoch 34/1000, Loss: 0.0013366081984713674\nEpoch 35/1000, Loss: 0.0014462199760600924\nEpoch 36/1000, Loss: 0.0011894946219399571\nEpoch 37/1000, Loss: 0.0010907946852967143\nEpoch 38/1000, Loss: 0.0013944971142336726\nEpoch 39/1000, Loss: 0.0013785740593448281\nEpoch 40/1000, Loss: 0.0010035333689302206\nEpoch 41/1000, Loss: 0.0010509977582842112\nEpoch 42/1000, Loss: 0.0011640426237136126\nEpoch 43/1000, Loss: 0.001244391081854701\nEpoch 44/1000, Loss: 0.0012282850220799446\nEpoch 45/1000, Loss: 0.0014695856953039765\nEpoch 46/1000, Loss: 0.0012455686228349805\nEpoch 47/1000, Loss: 0.0016521557699888945\nEpoch 48/1000, Loss: 0.0015459515852853656\nEpoch 49/1000, Loss: 0.0011986495228484273\nEpoch 50/1000, Loss: 0.0014065229333937168\nEpoch 51/1000, Loss: 0.0011895857751369476\nEpoch 52/1000, Loss: 0.0013814036501571536\nEpoch 53/1000, Loss: 0.0012524154735729098\nEpoch 54/1000, Loss: 0.001295537338592112\nEpoch 55/1000, Loss: 0.001563900150358677\nEpoch 56/1000, Loss: 0.0012664864771068096\nEpoch 57/1000, Loss: 0.001342339557595551\nEpoch 58/1000, Loss: 0.001336501445621252\nEpoch 59/1000, Loss: 0.0014931863406673074\nEpoch 60/1000, Loss: 0.0013498712796717882\nEpoch 61/1000, Loss: 0.0012192479334771633\nEpoch 62/1000, Loss: 0.001060851151123643\nEpoch 63/1000, Loss: 0.001199921709485352\nEpoch 64/1000, Loss: 0.0013339954894036055\nEpoch 65/1000, Loss: 0.0013374241534620523\nEpoch 66/1000, Loss: 0.0021433562505990267\nEpoch 67/1000, Loss: 0.0012539487797766924\nEpoch 68/1000, Loss: 0.0012648844858631492\nEpoch 69/1000, Loss: 0.0010719390120357275\nEpoch 70/1000, Loss: 0.0010984087130054832\nEpoch 71/1000, Loss: 0.001029699225910008\nEpoch 72/1000, Loss: 0.0013229447649791837\nEpoch 73/1000, Loss: 0.001361287897452712\nEpoch 74/1000, Loss: 0.0010532396845519543\nEpoch 75/1000, Loss: 0.0012430297210812569\nEpoch 76/1000, Loss: 0.001413110177963972\nEpoch 77/1000, Loss: 0.0009144890937022865\nEpoch 78/1000, Loss: 0.0010906979441642761\nEpoch 79/1000, Loss: 0.0014819352654740214\nEpoch 80/1000, Loss: 0.001321064424701035\nEpoch 81/1000, Loss: 0.0011906198924407363\nEpoch 82/1000, Loss: 0.000974946073256433\nEpoch 83/1000, Loss: 0.0009495850536040962\nEpoch 84/1000, Loss: 0.0014836806803941727\nEpoch 85/1000, Loss: 0.0010530150029808283\nEpoch 86/1000, Loss: 0.0010687030153349042\nEpoch 87/1000, Loss: 0.0013203562702983618\nEpoch 88/1000, Loss: 0.0013089878484606743\nEpoch 89/1000, Loss: 0.0008756344905123115\nEpoch 90/1000, Loss: 0.0009382284479215741\nEpoch 91/1000, Loss: 0.0010869668330997229\nEpoch 92/1000, Loss: 0.0010161740938201547\nEpoch 93/1000, Loss: 0.0009094257256947458\nEpoch 94/1000, Loss: 0.0012934673577547073\nEpoch 95/1000, Loss: 0.0012721915263682604\nEpoch 96/1000, Loss: 0.0007728053606115282\nEpoch 97/1000, Loss: 0.0014345756499096751\nEpoch 98/1000, Loss: 0.0009729791199788451\nEpoch 99/1000, Loss: 0.0008586706826463342\nEpoch 100/1000, Loss: 0.0013576572528108954\nEpoch 101/1000, Loss: 0.0010158373042941093\nEpoch 102/1000, Loss: 0.0010619526728987694\nEpoch 103/1000, Loss: 0.0008785834070295095\nEpoch 104/1000, Loss: 0.000895187258720398\nEpoch 105/1000, Loss: 0.0007543223327957094\nEpoch 106/1000, Loss: 0.0007677938556298614\nEpoch 107/1000, Loss: 0.000980766722932458\nEpoch 108/1000, Loss: 0.0014516992960125208\nEpoch 109/1000, Loss: 0.0011027012951672077\nEpoch 110/1000, Loss: 0.0009539329912513494\nEpoch 111/1000, Loss: 0.0013590725138783455\nEpoch 112/1000, Loss: 0.0008644118788652122\nEpoch 113/1000, Loss: 0.000736493500880897\nEpoch 114/1000, Loss: 0.0011210354277864099\nEpoch 115/1000, Loss: 0.0008741386700421572\nEpoch 116/1000, Loss: 0.001059511210769415\nEpoch 117/1000, Loss: 0.0009848616318777204\nEpoch 118/1000, Loss: 0.0011486097937449813\nEpoch 119/1000, Loss: 0.0008692769333720207\nEpoch 120/1000, Loss: 0.000851619930472225\nEpoch 121/1000, Loss: 0.001110077602788806\nEpoch 122/1000, Loss: 0.0010719957062974572\nEpoch 123/1000, Loss: 0.001464621163904667\nEpoch 124/1000, Loss: 0.0010723790619522333\nEpoch 125/1000, Loss: 0.0011614506365731359\nEpoch 126/1000, Loss: 0.0012176388408988714\nEpoch 127/1000, Loss: 0.001457095961086452\nEpoch 128/1000, Loss: 0.001692584017291665\nEpoch 129/1000, Loss: 0.0009206631220877171\nEpoch 130/1000, Loss: 0.0011308398097753525\nEpoch 131/1000, Loss: 0.0007351484964601696\nEpoch 132/1000, Loss: 0.0014819918433204293\nEpoch 133/1000, Loss: 0.0013978551141917706\nEpoch 134/1000, Loss: 0.00101259455550462\nEpoch 135/1000, Loss: 0.0008667763322591782\nEpoch 136/1000, Loss: 0.0021890837233513594\nEpoch 137/1000, Loss: 0.0011672104010358453\nEpoch 138/1000, Loss: 0.0011834121542051435\nEpoch 139/1000, Loss: 0.0007301326841115952\nEpoch 140/1000, Loss: 0.0008635675185360014\nEpoch 141/1000, Loss: 0.0008911729091778398\nEpoch 142/1000, Loss: 0.0008473697816953063\nEpoch 143/1000, Loss: 0.0015683467499911785\nEpoch 144/1000, Loss: 0.0009974411223083735\nEpoch 145/1000, Loss: 0.000982689089141786\nEpoch 146/1000, Loss: 0.0010330036748200655\nEpoch 147/1000, Loss: 0.0010476059978827834\nEpoch 148/1000, Loss: 0.0009213382145389915\nEpoch 149/1000, Loss: 0.000951785477809608\nEpoch 150/1000, Loss: 0.000920221209526062\nEpoch 151/1000, Loss: 0.0009625887032598257\nEpoch 152/1000, Loss: 0.0012578017776831985\nEpoch 153/1000, Loss: 0.0015311501920223236\nEpoch 154/1000, Loss: 0.0008954835357144475\nEpoch 155/1000, Loss: 0.0014113832730799913\nEpoch 156/1000, Loss: 0.0007017355528660119\nEpoch 157/1000, Loss: 0.0007730110082775354\nEpoch 158/1000, Loss: 0.0014378809137269855\nEpoch 159/1000, Loss: 0.0009731786558404565\nEpoch 160/1000, Loss: 0.0009096876019611955\nEpoch 161/1000, Loss: 0.0012469139182940125\nEpoch 162/1000, Loss: 0.0006543929921463132\nEpoch 163/1000, Loss: 0.0009444732568226755\nEpoch 164/1000, Loss: 0.0009675933397375047\nEpoch 165/1000, Loss: 0.0012310227612033486\nEpoch 166/1000, Loss: 0.0008342358050867915\nEpoch 167/1000, Loss: 0.000626876309979707\nEpoch 168/1000, Loss: 0.0012357045197859406\nEpoch 169/1000, Loss: 0.0007039308547973633\nEpoch 170/1000, Loss: 0.0008846877026371658\nEpoch 171/1000, Loss: 0.0016938173212110996\nEpoch 172/1000, Loss: 0.0009400080889463425\nEpoch 173/1000, Loss: 0.0010737177217379212\nEpoch 174/1000, Loss: 0.0009013421949930489\nEpoch 175/1000, Loss: 0.0007541602244600654\nEpoch 176/1000, Loss: 0.0013405551435425878\nEpoch 177/1000, Loss: 0.0009076861315406859\nEpoch 178/1000, Loss: 0.0011102918069809675\nEpoch 179/1000, Loss: 0.0008895481587387621\nEpoch 180/1000, Loss: 0.0009394976659677923\nEpoch 181/1000, Loss: 0.0007078600465320051\nEpoch 182/1000, Loss: 0.0008233434055000544\nEpoch 183/1000, Loss: 0.0006631356081925333\nEpoch 184/1000, Loss: 0.0008750914130359888\nEpoch 185/1000, Loss: 0.0009097338188439608\nEpoch 186/1000, Loss: 0.0008006920688785613\nEpoch 187/1000, Loss: 0.000740623741876334\nEpoch 188/1000, Loss: 0.0006555761210620403\nEpoch 189/1000, Loss: 0.0011600714642554522\nEpoch 190/1000, Loss: 0.0010827205842360854\nEpoch 191/1000, Loss: 0.0011710186954587698\nEpoch 192/1000, Loss: 0.00158397713676095\nEpoch 193/1000, Loss: 0.0011106942547485232\nEpoch 194/1000, Loss: 0.0016243749996647239\nEpoch 195/1000, Loss: 0.0007941903895698488\nEpoch 196/1000, Loss: 0.0006958068115636706\nEpoch 197/1000, Loss: 0.0008074299548752606\nEpoch 198/1000, Loss: 0.001181484665721655\nEpoch 199/1000, Loss: 0.0007683601579628885\nEpoch 200/1000, Loss: 0.001279168762266636\nEpoch 201/1000, Loss: 0.0011984118027612567\nEpoch 202/1000, Loss: 0.0006339778192341328\nEpoch 203/1000, Loss: 0.0006803550641052425\nEpoch 204/1000, Loss: 0.000909327354747802\nEpoch 205/1000, Loss: 0.0008486915612593293\nEpoch 206/1000, Loss: 0.0007976273773238063\nEpoch 207/1000, Loss: 0.0011054915376007557\nEpoch 208/1000, Loss: 0.000658490345813334\nEpoch 209/1000, Loss: 0.0010990716982632875\nEpoch 210/1000, Loss: 0.0011192485690116882\nEpoch 211/1000, Loss: 0.0008233872940763831\nEpoch 212/1000, Loss: 0.0007221349515020847\nEpoch 213/1000, Loss: 0.0008622679160907865\nEpoch 214/1000, Loss: 0.0008984577725641429\nEpoch 215/1000, Loss: 0.0007026653038337827\nEpoch 216/1000, Loss: 0.0008980646962299943\nEpoch 217/1000, Loss: 0.0006576059968210757\nEpoch 218/1000, Loss: 0.000763074669521302\nEpoch 219/1000, Loss: 0.001074054162018001\nEpoch 220/1000, Loss: 0.0008177227573469281\nEpoch 221/1000, Loss: 0.0008298186003230512\nEpoch 222/1000, Loss: 0.0012293398613110185\nEpoch 223/1000, Loss: 0.0006395873497240245\nEpoch 224/1000, Loss: 0.0008305017254315317\nEpoch 225/1000, Loss: 0.0008001982350833714\nEpoch 226/1000, Loss: 0.0015491315862163901\nEpoch 227/1000, Loss: 0.0006865934119559824\nEpoch 228/1000, Loss: 0.0005467539303936064\nEpoch 229/1000, Loss: 0.0011020017554983497\nEpoch 230/1000, Loss: 0.0007963604293763638\nEpoch 231/1000, Loss: 0.0006478921277448535\nEpoch 232/1000, Loss: 0.0007800046587362885\nEpoch 233/1000, Loss: 0.0011958983959630132\nEpoch 234/1000, Loss: 0.0006845098687335849\nEpoch 235/1000, Loss: 0.0006077023572288454\nEpoch 236/1000, Loss: 0.0006864143651910126\nEpoch 237/1000, Loss: 0.0009073659311980009\nEpoch 238/1000, Loss: 0.0006923253531567752\nEpoch 239/1000, Loss: 0.0007426194497384131\nEpoch 240/1000, Loss: 0.001105171162635088\nEpoch 241/1000, Loss: 0.0006265747360885143\nEpoch 242/1000, Loss: 0.0007433270220644772\nEpoch 243/1000, Loss: 0.0007317003328353167\nEpoch 244/1000, Loss: 0.0005646508070640266\nEpoch 245/1000, Loss: 0.0007546356064267457\nEpoch 246/1000, Loss: 0.0006816519307903945\nEpoch 247/1000, Loss: 0.0007504221866838634\nEpoch 248/1000, Loss: 0.0007122289389371872\nEpoch 249/1000, Loss: 0.0009836133103817701\nEpoch 250/1000, Loss: 0.0008211949607357383\nEpoch 251/1000, Loss: 0.0015502896858379245\nEpoch 252/1000, Loss: 0.0012416112003847957\nEpoch 253/1000, Loss: 0.0006103601772338152\nEpoch 254/1000, Loss: 0.0010185902938246727\nEpoch 255/1000, Loss: 0.0005042601260356605\nEpoch 256/1000, Loss: 0.0007473398582078516\nEpoch 257/1000, Loss: 0.0006421855068765581\nEpoch 258/1000, Loss: 0.0011296620359644294\nEpoch 259/1000, Loss: 0.0007034486043266952\nEpoch 260/1000, Loss: 0.0009826394962146878\nEpoch 261/1000, Loss: 0.0006964944768697023\nEpoch 262/1000, Loss: 0.0007527681300416589\nEpoch 263/1000, Loss: 0.0009027009946294129\nEpoch 264/1000, Loss: 0.0006380985723808408\nEpoch 265/1000, Loss: 0.000859789433889091\nEpoch 266/1000, Loss: 0.0008083320572040975\nEpoch 267/1000, Loss: 0.0007176608196459711\nEpoch 268/1000, Loss: 0.002069533336907625\nEpoch 269/1000, Loss: 0.0006889033829793334\nEpoch 270/1000, Loss: 0.0013211137847974896\nEpoch 271/1000, Loss: 0.0007932479493319988\nEpoch 272/1000, Loss: 0.0008103224681690335\nEpoch 273/1000, Loss: 0.0007788792136125267\nEpoch 274/1000, Loss: 0.0008194817928597331\nEpoch 275/1000, Loss: 0.0007531727897003293\nEpoch 276/1000, Loss: 0.0011188822099938989\nEpoch 277/1000, Loss: 0.0006616966566070914\nEpoch 278/1000, Loss: 0.0011774920858442783\nEpoch 279/1000, Loss: 0.0007067936239764094\nEpoch 280/1000, Loss: 0.0008587418124079704\nEpoch 281/1000, Loss: 0.0007649676408618689\nEpoch 282/1000, Loss: 0.0008230459061451256\nEpoch 283/1000, Loss: 0.0011285679647698998\nEpoch 284/1000, Loss: 0.0009968291269615293\nEpoch 285/1000, Loss: 0.000899329490493983\nEpoch 286/1000, Loss: 0.0007950340514071286\nEpoch 287/1000, Loss: 0.0009103553020395339\nEpoch 288/1000, Loss: 0.0006002939771860838\nEpoch 289/1000, Loss: 0.0006888706702739\nEpoch 290/1000, Loss: 0.0014753332361578941\nEpoch 291/1000, Loss: 0.0006800605915486813\nEpoch 292/1000, Loss: 0.0009262486128136516\nEpoch 293/1000, Loss: 0.0007618828676640987\nEpoch 294/1000, Loss: 0.0006368815083988011\nEpoch 295/1000, Loss: 0.0007027637329883873\nEpoch 296/1000, Loss: 0.0006289492594078183\nEpoch 297/1000, Loss: 0.0014908096054568887\nEpoch 298/1000, Loss: 0.0011083126300945878\nEpoch 299/1000, Loss: 0.0005081709823571146\nEpoch 300/1000, Loss: 0.0006903553148731589\nEpoch 301/1000, Loss: 0.0005653721163980663\nEpoch 302/1000, Loss: 0.001666621770709753\nEpoch 303/1000, Loss: 0.000571847427636385\nEpoch 304/1000, Loss: 0.000651322421617806\nEpoch 305/1000, Loss: 0.0006443510064855218\nEpoch 306/1000, Loss: 0.0009431316866539419\nEpoch 307/1000, Loss: 0.0012537799775600433\nEpoch 308/1000, Loss: 0.000542033405508846\nEpoch 309/1000, Loss: 0.000819826906081289\nEpoch 310/1000, Loss: 0.0005994070088490844\nEpoch 311/1000, Loss: 0.0007115549524314702\nEpoch 312/1000, Loss: 0.0009514586417935789\nEpoch 313/1000, Loss: 0.0006142060738056898\nEpoch 314/1000, Loss: 0.0007884184597060084\nEpoch 315/1000, Loss: 0.0005079805268906057\nEpoch 316/1000, Loss: 0.0005625727935694158\nEpoch 317/1000, Loss: 0.0010734281968325377\nEpoch 318/1000, Loss: 0.0008890003664419055\nEpoch 319/1000, Loss: 0.0005204781191423535\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[123], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m output \u001b[38;5;241m=\u001b[39m model(encoder_inputs, decoder_inputs)\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), decoder_targets\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     27\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# Saving model parameters\ntorch.save(model.state_dict(), \".../transformer_model.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-06-07T21:39:03.421503Z","iopub.execute_input":"2023-06-07T21:39:03.422005Z","iopub.status.idle":"2023-06-07T21:39:03.525044Z","shell.execute_reply.started":"2023-06-07T21:39:03.421968Z","shell.execute_reply":"2023-06-07T21:39:03.523286Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"markdown","source":"# Model Inference","metadata":{}},{"cell_type":"code","source":"# Inference function\ndef transformer_inference(input_text, max_output_len):\n    model.eval()\n\n    input_text = input_text.lower()\n\n    input_tokens = encode(input_text)\n    input_tensor = torch.tensor(input_tokens).unsqueeze(0) \n    \n    sos_token = char2idx['<SOS>']\n    decoder_inputs = torch.tensor([sos_token]).unsqueeze(0)\n\n    idx = decoder_inputs.to(device)\n    input_tensor = input_tensor.to(device)\n    while True:\n        with torch.no_grad(): \n            output = model(input_tensor, idx)\n\n        logits = output[:, -1, :]\n        probs = F.softmax(logits, dim=-1)\n#       Sampling one token according to the probability distribution predicted by the model\n        idx_next = torch.multinomial(probs, num_samples=1)\n#       Concatenate the previous model decoder input with the generated token\n        idx = torch.cat((idx, idx_next), dim=1)\n        \n        if int(idx_next.item()) == char2idx['<EOS>'] or len(idx[0].tolist()) > max_output_len:\n            break\n    return decode(idx[0])","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:11:04.917902Z","iopub.execute_input":"2023-06-08T00:11:04.918285Z","iopub.status.idle":"2023-06-08T00:11:04.926730Z","shell.execute_reply.started":"2023-06-08T00:11:04.918254Z","shell.execute_reply":"2023-06-08T00:11:04.925841Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"transformer_inference(\"i am not stupid enough to repeat your mistakes\", 50)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:11:09.192224Z","iopub.execute_input":"2023-06-08T00:11:09.193366Z","iopub.status.idle":"2023-06-08T00:11:09.405822Z","shell.execute_reply.started":"2023-06-08T00:11:09.193322Z","shell.execute_reply":"2023-06-08T00:11:09.404931Z"},"trusted":true},"execution_count":127,"outputs":[{"execution_count":127,"output_type":"execute_result","data":{"text/plain":"'<SOS>wt<PAD>p<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>'"},"metadata":{}}]}]}