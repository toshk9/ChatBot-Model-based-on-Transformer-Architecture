{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis notebook presents a version of the model based on the transformer architecture. The model is trained on a dataset consisting of a variety of dialogs. At the end of the notebook is the model inference code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nfrom torch.nn.utils.rnn import pad_sequence\n\nimport numpy as np\nimport codecs\nimport math\nimport re","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:46.635005Z","iopub.execute_input":"2023-06-08T00:41:46.635362Z","iopub.status.idle":"2023-06-08T00:41:49.729116Z","shell.execute_reply.started":"2023-06-08T00:41:46.635333Z","shell.execute_reply":"2023-06-08T00:41:49.728171Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data Preprocessing Pipeline","metadata":{}},{"cell_type":"code","source":"# READING THE DATA\nlines = open('.../Chatbot/movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\nconversations = open('.../Chatbot/movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:49.731088Z","iopub.execute_input":"2023-06-08T00:41:49.732443Z","iopub.status.idle":"2023-06-08T00:41:50.247889Z","shell.execute_reply.started":"2023-06-08T00:41:49.732389Z","shell.execute_reply":"2023-06-08T00:41:50.244479Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# READING THE DATA\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m.../Chatbot/movie_lines.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m conversations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.../Chatbot/movie_conversations.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m, errors \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.../Chatbot/movie_lines.txt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '.../Chatbot/movie_lines.txt'","output_type":"error"}]},{"cell_type":"code","source":"# Creating a dictionary that maps each line and its id\nid2line = {}\nfor line in lines:\n    _line = line.split(' +++$+++ ')\n    if len(_line) == 5:\n        id2line[_line[0]] = _line[4]","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.248741Z","iopub.status.idle":"2023-06-08T00:41:50.249090Z","shell.execute_reply.started":"2023-06-08T00:41:50.248919Z","shell.execute_reply":"2023-06-08T00:41:50.248934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a list of all of the conversations\nconversations_ids = []\nfor conversation in conversations[:-1]:\n    _conversation = conversation.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \"\").replace(\" \", \"\")\n    conversations_ids.append(_conversation.split(','))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.251211Z","iopub.status.idle":"2023-06-08T00:41:50.251671Z","shell.execute_reply.started":"2023-06-08T00:41:50.251444Z","shell.execute_reply":"2023-06-08T00:41:50.251466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Getting separately the questions and the answers\nquestions = []\nanswers = []\nfor conversation in conversations_ids:\n    for i in range(len(conversation) - 1):\n        questions.append(id2line[conversation[i]])\n        answers.append(id2line[conversation[i+1]])","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.258548Z","iopub.status.idle":"2023-06-08T00:41:50.258894Z","shell.execute_reply.started":"2023-06-08T00:41:50.258731Z","shell.execute_reply":"2023-06-08T00:41:50.258746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Doing a first cleaning of the texts\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"i'm\", \"i am\", text)\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"she's\", \"she is\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"how's\", \"how is\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"n't\", \" not\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.259962Z","iopub.status.idle":"2023-06-08T00:41:50.260292Z","shell.execute_reply.started":"2023-06-08T00:41:50.260132Z","shell.execute_reply":"2023-06-08T00:41:50.260147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning the questions\nclean_questions = []\nfor question in questions:\n    clean_questions.append(clean_text(question))\n\n# Cleaning the answers\nclean_answers = []\nfor answer in answers:\n    clean_answers.append(clean_text(answer))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.261337Z","iopub.status.idle":"2023-06-08T00:41:50.261678Z","shell.execute_reply.started":"2023-06-08T00:41:50.261520Z","shell.execute_reply":"2023-06-08T00:41:50.261535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filtering out the questions and answers that are too short or too long\nshort_questions = []\nshort_answers = []\ni = 0\nfor question in clean_questions:\n    if 2 <= len(question.split()) <= 25:\n        short_questions.append(question)\n        short_answers.append(clean_answers[i])\n    i += 1\nclean_questions = []\nclean_answers = []\ni = 0\nfor answer in short_answers:\n    if 2 <= len(answer.split()) <= 25:\n        clean_answers.append(answer)\n        clean_questions.append(short_questions[i])\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.262944Z","iopub.status.idle":"2023-06-08T00:41:50.263275Z","shell.execute_reply.started":"2023-06-08T00:41:50.263117Z","shell.execute_reply":"2023-06-08T00:41:50.263132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We create a dictionary of matching index and word and the reverse of it.\nvocab = sorted(list(set(''.join(clean_questions + clean_answers))))\n#  Add special tokens\nvocab.append(\"<SOS>\")\nvocab.append(\"<EOS>\")\nvocab.append(\"<PAD>\")\nvocab.append(\"<OUT>\")\nVOCAB_SIZE = len(vocab)\n\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = {i:u for i, u in enumerate(vocab)}\n# Create encoding and decoding functions\nencode = lambda s: [char2idx[c] for c in s]\ndecode = lambda l: ''.join([idx2char[int(i)] for i in l])","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.266541Z","iopub.status.idle":"2023-06-08T00:41:50.267194Z","shell.execute_reply.started":"2023-06-08T00:41:50.266962Z","shell.execute_reply":"2023-06-08T00:41:50.266983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Translate all question and answer characters in dialogs into indexes\ninput_questions_as_int = [encode(question) for question in clean_questions]\ntarget_answers_as_int = [encode(answer) for answer in clean_answers]\n\nencoder_input_seqs = input_questions_as_int\ndecoder_input_seqs = []\ndecoder_target_seqs = []\n# Add the special token indices to the decoder_input_seqs and the decoder_target_seqs\nfor target_text in target_answers_as_int:\n    decoder_input_seqs.append([char2idx['<SOS>']] + target_text)\n    decoder_target_seqs.append(target_text + [char2idx['<EOS>']])","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.268397Z","iopub.status.idle":"2023-06-08T00:41:50.269081Z","shell.execute_reply.started":"2023-06-08T00:41:50.268846Z","shell.execute_reply":"2023-06-08T00:41:50.268868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# determine the maximum number of sequences (question-answer) of data for training \nSEQS = 15000\n\nencoder_input_seqs = encoder_input_seqs[:SEQS]\ndecoder_input_seqs = decoder_input_seqs[:SEQS]\ndecoder_target_seqs = decoder_target_seqs[:SEQS]","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.270286Z","iopub.status.idle":"2023-06-08T00:41:50.270972Z","shell.execute_reply.started":"2023-06-08T00:41:50.270743Z","shell.execute_reply":"2023-06-08T00:41:50.270766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Class for pytorch DataLoader \nclass ChatDataset(Dataset):\n    def __init__(self, encoder_input_seqs, decoder_input_seqs, decoder_target_seqs):\n        self.encoder_input_seqs = encoder_input_seqs\n        self.decoder_input_seqs = decoder_input_seqs\n        self.decoder_target_seqs = decoder_target_seqs\n\n    def __len__(self):\n        return len(self.encoder_input_seqs)\n\n    def __getitem__(self, idx):\n        encoder_input = self.encoder_input_seqs[idx]\n        decoder_input = self.decoder_input_seqs[idx]\n        decoder_target = self.decoder_target_seqs[idx]\n        return encoder_input, decoder_input, decoder_target\n# Function for converting lists into tensors and converting all encoder_inputs, \n# decoder_inputs, decoder_targets sequences to the same length of each group using paddings\npadding_value = char2idx['<PAD>']\ndef collate_fn(batch, padding_value=char2idx['<PAD>']):\n    encoder_inputs, decoder_inputs, decoder_targets = zip(*batch)\n    encoder_inputs = [torch.LongTensor(seq) for seq in encoder_inputs]\n    decoder_inputs = [torch.LongTensor(seq) for seq in decoder_inputs]\n    decoder_targets = [torch.LongTensor(seq) for seq in decoder_targets]\n\n    d_list = encoder_inputs + decoder_inputs + decoder_targets\n    d_list_padded = pad_sequence(d_list, batch_first=True, padding_value=padding_value)\n    encoder_inputs = d_list_padded[:int(1/3 * len(d_list_padded))]\n    decoder_inputs = d_list_padded[int(1/3 * len(d_list_padded)):int(2/3 * len(d_list_padded))]\n    decoder_targets = d_list_padded[int(2/3 * len(d_list_padded)):]\n\n    return encoder_inputs, decoder_inputs, decoder_targets","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.272175Z","iopub.status.idle":"2023-06-08T00:41:50.272855Z","shell.execute_reply.started":"2023-06-08T00:41:50.272624Z","shell.execute_reply":"2023-06-08T00:41:50.272646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mask creation func\ndef create_self_attention_mask(sequence_length):\n    # Create a square matrix (sequence_length, sequence_length)\n    mask = torch.triu(torch.ones(sequence_length, sequence_length), diagonal=1).bool() \n    return mask","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.274063Z","iopub.status.idle":"2023-06-08T00:41:50.274735Z","shell.execute_reply.started":"2023-06-08T00:41:50.274503Z","shell.execute_reply":"2023-06-08T00:41:50.274525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size, num_layers, num_heads, dropout):\n        super().__init__()\n        self.encoder = TransformerEncoder(input_size, hidden_size, num_layers, num_heads, dropout)\n        self.decoder = TransformerDecoder(input_size, hidden_size, num_layers, num_heads, dropout)\n        self.output_linear = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, source, target):\n        encoder_output = self.encoder(source)\n        decoder_output = self.decoder(target, encoder_output)\n        output = self.output_linear(decoder_output)\n        return output\n    \nclass TransformerEncoder(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_heads, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.pos_encoding = PositionalEncoding(hidden_size, dropout)\n        self.encoder_layers = nn.ModuleList([\n            TransformerEncoderLayer(hidden_size, num_heads, dropout)\n            for _ in range(num_layers)\n        ])\n    \n    def forward(self, x):\n             \n        out = self.embedding(x)\n        out = self.pos_encoding(out)\n        \n        for layer in self.encoder_layers:\n            out = layer(out)\n\n        return out\n    \nclass TransformerDecoder(nn.Module):\n    def __init__(self, output_size, hidden_size, num_layers, num_heads, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(output_size, hidden_size)\n        self.pos_encoding = PositionalEncoding(hidden_size, dropout)\n        self.decoder_layers = nn.ModuleList([\n            TransformerDecoderLayer(hidden_size, num_heads, dropout)\n            for _ in range(num_layers)\n        ])\n        \n    def forward(self, x, encoder_output):\n\n        out = self.embedding(x)\n        out = self.pos_encoding(out)\n        \n        mask = create_self_attention_mask(x.size()[1])\n        mask = mask.to(device)\n        \n        for layer in self.decoder_layers:\n            out = layer(out, encoder_output, self_attention_mask=mask)\n\n        return out \n\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, hidden_size, num_heads, dropout):\n        super().__init__()\n        self.self_attention = MultiHeadAttention(hidden_size, num_heads)\n        self.feed_forward = FeedForward(hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm1 = nn.LayerNorm(hidden_size)\n        self.layer_norm2 = nn.LayerNorm(hidden_size)\n    \n    def forward(self, x):\n\n        residual = x # for Add & Norm 1\n        out = self.dropout(self.self_attention(x, x, x))\n        out = residual + out # Add\n        out = self.layer_norm1(out) # Norm\n        \n        residual = out # for Add & Norm 2\n        out = self.dropout(self.feed_forward(out))\n        out = residual + out # Add\n        out = self.layer_norm2(out) # Norm\n\n        return out\n    \nclass TransformerDecoderLayer(nn.Module):\n    def __init__(self, hidden_size, num_heads, dropout):\n        super().__init__()\n        self.self_attention = MultiHeadAttention(hidden_size, num_heads)\n        self.encoder_attention = MultiHeadAttention(hidden_size, num_heads)\n        self.feed_forward = FeedForward(hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm1 = nn.LayerNorm(hidden_size)\n        self.layer_norm2 = nn.LayerNorm(hidden_size)        \n        self.layer_norm3 = nn.LayerNorm(hidden_size)    \n        \n    def forward(self, x, encoder_output, self_attention_mask=None):\n\n        residual = x # for Add & Norm 1\n        out = self.dropout(self.self_attention(x, x, x, mask=self_attention_mask))\n        out = residual + out # Add \n        out = self.layer_norm1(out) # Norm\n        \n        residual = out # for Add & Norm 2\n        out = self.dropout(self.encoder_attention(x, encoder_output, encoder_output)) # СДЕЛАТЬ ПОМЕТКУ q, k, v\n        out = residual + out # Add\n        out = self.layer_norm2(out) # Norm\n        \n        residual = out # for Add & Norm 3\n        out = self.dropout(self.feed_forward(out))\n        out = residual + out # Add\n        out = self.layer_norm3(out) # Norm\n\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, hidden_size, num_heads):\n        super().__init__()\n        self.num_heads = num_heads\n        self.hidden_size = hidden_size\n        self.head_size = hidden_size // num_heads\n        \n        self.q_linear = nn.Linear(hidden_size, hidden_size) \n        self.k_linear = nn.Linear(hidden_size, hidden_size) \n        self.v_linear = nn.Linear(hidden_size, hidden_size) \n        self.output_linear = nn.Linear(hidden_size, hidden_size) \n    \n    def forward(self, query, key, value, mask=None):\n\n        batch_size = query.size(0)\n        \n        q = self.q_linear(query)\n        k = self.k_linear(key)\n        v = self.v_linear(value)\n        \n        q = self._split_heads(q) \n        k = self._split_heads(k)\n        v = self._split_heads(v)\n\n        scores  = torch.matmul(q, k.transpose(-2, -1)) # (-2, -1) dimensions that need to be swapped out \n        scores = scores / (self.head_size ** 0.5) \n        \n        if mask is not None:\n            \n            scores = scores.masked_fill(mask == 0, -1e9)\n        \n        scores = F.softmax(scores, dim=-1) \n        \n        attention = torch.matmul(scores, v) \n        attention = self._combine_heads(attention) # сoncatenation of all self-attention heads\n        attention = self.output_linear(attention) \n        \n        return attention\n    \n    # Dividing the hidden_size dimension into self.num_heads and self.heads_size\n    def _split_heads(self, x):\n\n        batch_size, sequence_length, hidden_size = x.size()\n        x = x.view(batch_size, sequence_length, self.num_heads, self.head_size) # .view() change the dimensions without changing the contents\n        x = x.transpose(1, 2)\n        x = x.contiguous().view(batch_size * self.num_heads, sequence_length, self.head_size) # .contiguous() create a new tensor with the same content, but with guaranteed consistent placement of elements in memory.\n\n        return x\n    # Combining the dimensions of self.num_heads and self.heads_size into hidden_size\n    def _combine_heads(self, x):\n        batch_size, sequence_length, head_size = x.size()\n        x = x.view(batch_size // self.num_heads, sequence_length, self.num_heads * head_size)\n        x = x.transpose(1, 2)\n        x = x.contiguous().view(batch_size // self.num_heads, sequence_length, self.hidden_size) \n        return x\n    \nclass FeedForward(nn.Module):\n    def __init__(self, hidden_size):\n        super().__init__()\n        self.linear1 = nn.Linear(hidden_size, hidden_size)\n        self.linear2 = nn.Linear(hidden_size, hidden_size)\n        \n    def forward(self, x):\n        out = F.relu(self.linear1(x))\n        out = self.linear2(out)\n        return out\n    \nclass PositionalEncoding(nn.Module):\n    def __init__(self, hidden_size, dropout, max_length=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(dropout)\n        \n        position = torch.arange(0, max_length).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, hidden_size, 2) * -(math.log(10000.0) / hidden_size))\n        pe = torch.zeros(max_length, hidden_size)\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)]\n        return self.dropout(x)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.276044Z","iopub.status.idle":"2023-06-08T00:41:50.276725Z","shell.execute_reply.started":"2023-06-08T00:41:50.276493Z","shell.execute_reply":"2023-06-08T00:41:50.276515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define model and training parameters\ninput_size = VOCAB_SIZE\noutput_size = VOCAB_SIZE\nhidden_size = 512\nnum_layers = 2\nnum_heads = 2\ndropout = 0.1\nlearning_rate = 0.001\nnum_epochs = 1000\nbatch_size = 256","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.277923Z","iopub.status.idle":"2023-06-08T00:41:50.278608Z","shell.execute_reply.started":"2023-06-08T00:41:50.278349Z","shell.execute_reply":"2023-06-08T00:41:50.278371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The model includes two encoder parts and two decoders (num_layers). The number of \"heads\" in the self-attention mechanisms in the model is 2. In addition, we add 0.1 dropout in some network layers. ","metadata":{}},{"cell_type":"code","source":"model = Transformer(input_size, output_size, hidden_size, num_layers, num_heads, dropout)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.279801Z","iopub.status.idle":"2023-06-08T00:41:50.280481Z","shell.execute_reply.started":"2023-06-08T00:41:50.280222Z","shell.execute_reply":"2023-06-08T00:41:50.280244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading model parameters from a file\nmodel.load_state_dict(torch.load('.../transformer_model.pth', map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.281677Z","iopub.status.idle":"2023-06-08T00:41:50.282328Z","shell.execute_reply.started":"2023-06-08T00:41:50.282095Z","shell.execute_reply":"2023-06-08T00:41:50.282117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# Define the loss function and the optimizer\n\ncriterion = nn.CrossEntropyLoss(ignore_index=padding_value)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Create an instance of dataset and data loader\ndataset = ChatDataset(encoder_input_seqs, decoder_input_seqs, decoder_target_seqs)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n# Define the device on which the model will be trained\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.283556Z","iopub.status.idle":"2023-06-08T00:41:50.284211Z","shell.execute_reply.started":"2023-06-08T00:41:50.283976Z","shell.execute_reply":"2023-06-08T00:41:50.283998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put the model in training mode\nmodel.train()\n\n# Learning Cycle\nfor epoch in range(num_epochs):\n    total_loss = 0\n\n    for encoder_inputs, decoder_inputs, decoder_targets in dataloader:\n\n        optimizer.zero_grad()\n\n#         Moving the training data to the device\n        encoder_inputs = encoder_inputs.to(device)\n        decoder_inputs = decoder_inputs.to(device)\n        decoder_targets = decoder_targets.to(device)\n\n        output = model(encoder_inputs, decoder_inputs)\n\n        loss = criterion(output.view(-1, output.size(-1)), decoder_targets.view(-1))\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.285410Z","iopub.status.idle":"2023-06-08T00:41:50.286090Z","shell.execute_reply.started":"2023-06-08T00:41:50.285856Z","shell.execute_reply":"2023-06-08T00:41:50.285877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving model parameters\ntorch.save(model.state_dict(), \".../transformer_model.pth\")","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.287272Z","iopub.status.idle":"2023-06-08T00:41:50.287946Z","shell.execute_reply.started":"2023-06-08T00:41:50.287712Z","shell.execute_reply":"2023-06-08T00:41:50.287734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Inference","metadata":{}},{"cell_type":"code","source":"# Inference function\ndef transformer_inference(input_text, max_output_len):\n    model.eval()\n\n    input_text = input_text.lower()\n\n    input_tokens = encode(input_text)\n    input_tensor = torch.tensor(input_tokens).unsqueeze(0) \n    \n    sos_token = char2idx['<SOS>']\n    decoder_inputs = torch.tensor([sos_token]).unsqueeze(0)\n\n    idx = decoder_inputs.to(device)\n    input_tensor = input_tensor.to(device)\n    while True:\n        with torch.no_grad(): \n            output = model(input_tensor, idx)\n\n        logits = output[:, -1, :]\n        probs = F.softmax(logits, dim=-1)\n#       Sampling one token according to the probability distribution predicted by the model\n        idx_next = torch.multinomial(probs, num_samples=1)\n#       Concatenate the previous model decoder input with the generated token\n        idx = torch.cat((idx, idx_next), dim=1)\n        \n        if int(idx_next.item()) == char2idx['<EOS>'] or len(idx[0].tolist()) > max_output_len:\n            break\n    return decode(idx[0])","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.289153Z","iopub.status.idle":"2023-06-08T00:41:50.289831Z","shell.execute_reply.started":"2023-06-08T00:41:50.289597Z","shell.execute_reply":"2023-06-08T00:41:50.289618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# INFERENCE EXAMPLE\ntransformer_inference(\"i am not stupid enough to repeat your mistakes\", 50)","metadata":{"execution":{"iopub.status.busy":"2023-06-08T00:41:50.291027Z","iopub.status.idle":"2023-06-08T00:41:50.291703Z","shell.execute_reply.started":"2023-06-08T00:41:50.291465Z","shell.execute_reply":"2023-06-08T00:41:50.291495Z"},"trusted":true},"execution_count":null,"outputs":[]}]}